(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{369:function(e,t,o){e.exports=o.p+"assets/img/req-limit.6e35ed6f.png"},370:function(e,t,o){e.exports=o.p+"assets/img/requests.a511950b.png"},371:function(e,t,o){e.exports=o.p+"assets/img/limits.f018b061.png"},417:function(e,t,o){"use strict";o.r(t);var s=o(42),i=Object(s.a)({},(function(){var e=this,t=e.$createElement,s=e._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[s("h1",{attrs:{id:"kubernetes-requests-limits"}},[e._v("Kubernetes Requests & Limits")]),e._v(" "),s("h2",{attrs:{id:"the-basics"}},[e._v("The Basics")]),e._v(" "),s("p",[s("img",{attrs:{src:o(369),alt:"Requests"}})]),e._v(" "),s("h3",{attrs:{id:"requests"}},[e._v("Requests")]),e._v(" "),s("p",[e._v("Pods will get the amount of memory/CPU they request. The requests specification is used at pod placement time: Kubernetes will look for a node that has both enough CPU and memory according to the requests configuration. If they exceed their memory request, they could be killed if another pod happens to need this memory. Pods are only ever killed when using less memory than requested if critical system or high priority workloads need the memory.")]),e._v(" "),s("h3",{attrs:{id:"limits"}},[e._v("Limits")]),e._v(" "),s("p",[e._v("Limits are enforced at runtime. If a container exceeds the limits, Kubernetes will try to stop it. For CPU, it will simply curb the usage so a container typically can't exceed its limit capacity ; it won't be killed, just won't be able to use more CPU. Pods will be CPU throttled when they exceed their CPU limit. If no limit is set, then the pods can use excess memory and CPU when available. If a container exceeds its memory limits, it would be terminated.")]),e._v(" "),s("h3",{attrs:{id:"compressible-resources"}},[e._v("Compressible Resources")]),e._v(" "),s("p",[e._v("CPU is considered a “compressible” resource while memory is “non-compressible”.")]),e._v(" "),s("p",[e._v("Compressible means we can squeeze more out of it. Pods can work with less of the resource although they would like to use more of it. For example, if you deploy a pod with a request of 1 CPU and no limit, it can use more than that if available. But when other pods on the same node get busy, they will have to share the available CPUs and might get throttled back to their request. However, they won’t be evicted and can still do their job.")]),e._v(" "),s("p",[e._v("For memory on the other hand, when a pod has a resource request for memory but no limit, it also might use more RAM than requested. However, when resources get low, it can’t be throttled back to use only the requested amount of memory and free up the rest. There is a possiblity that Kubernetes will evict such pods. Therefore it is crucial to always set a memory resource limit and take care that your microservice will never exceed that limit.")]),e._v(" "),s("p",[e._v("Similarly, even if you set limits CPU & memory on a pod, and pod reaches the limit CPU, the container will not get killed, rather it will be CPU throttled causing slowness. But if it reaches the memory limit, kubelet will kill the container stating OOMKilled(Out of memory killed)")]),e._v(" "),s("h3",{attrs:{id:"requests-is-a-guarantee-limits-is-an-obligation"}},[e._v("Requests is a guarantee, Limits is an obligation")]),e._v(" "),s("p",[e._v("There is a subtle change of semantics when we go from requests to limits. For the application developer, requests is a guarantee offered by Kubernetes that any pod scheduled will have at least the minumum amount of memory. limits is an obligation to stay under the maximum amount of memory, which will be enforced by the kernel.")]),e._v(" "),s("p",[e._v("In other words: containers can’t rely on being able to grow from their initial requests capacity to the maximum allowance set in limits.")]),e._v(" "),s("h2",{attrs:{id:"quality-of-service"}},[e._v("Quality of Service")]),e._v(" "),s("p",[e._v("QoS value for a pod determines how good the service of that pod will be. QoS can have three values")]),e._v(" "),s("h3",{attrs:{id:"_1-besteffort"}},[e._v("1. BestEffort")]),e._v(" "),s("p",[e._v("The QoS when you set no Requests & Limits on your pods.")]),e._v(" "),s("p",[e._v("When you declare no resources, Kubernetes has no idea where to put the pod. Some nodes are very busy, some nodes are very quiet, but since you’ve not told your cluster how much room your deployment needs, it is forced to guess.")]),e._v(" "),s("p",[e._v("It won’t just starve your other pods, it can also going to escalate and consume the CPU that would otherwise be allocated for really important things, like the kubelet. Your node can go dark and on some platforms, it can be more than 15 minutes before your cluster self-heals.")]),e._v(" "),s("h3",{attrs:{id:"_2-burstable"}},[e._v("2. Burstable")]),e._v(" "),s("p",[e._v("The QoS when you set Limits is higher than Requests on your pods.")]),e._v(" "),s("p",[e._v("Better than "),s("code",[e._v("BestEffort")]),e._v(" but if all of your pods have limits higher than your requests, and all/some of them grow simulatenously, they are overcommitting the CPU/Memory from the node. The issue is again all the pods are using more memory and can again commit more memory & CPU from the node, resulting in node failure or OOMKill of your containers. If the upper bound of all of your pods is several times higher than the capacity of the node on which they’re running, you’ve potentially got a problem.")]),e._v(" "),s("h3",{attrs:{id:"_3-guaranteed"}},[e._v("3. Guaranteed")]),e._v(" "),s("p",[e._v("The QoS when you set Limits are equal to Requests on your pods.")]),e._v(" "),s("p",[e._v("It is the most expensive way to run your containers, but those containers gain an awful lot of stability. Each pod will schedule on a node that has enough resources and will never throttle the node but if it can be expensive as you would have to set a higher number for requests & limits.")]),e._v(" "),s("h3",{attrs:{id:"conclusion"}},[e._v("Conclusion")]),e._v(" "),s("p",[e._v("This is a bit of a value judgment. Some pods sit idle for a long time and, perhaps for an hour, they get busy. It doesn’t make sense to hold onto resources for the other 23 hours as Guaranteed QoS does. So you make the choice. Risk vs. reward. If the pod self-heals quickly and you can tolerate short outages, Burstable might be a good method for you to save some money.")]),e._v(" "),s("h2",{attrs:{id:"the-tradeoffs"}},[e._v("The Tradeoffs")]),e._v(" "),s("h3",{attrs:{id:"requests-2"}},[e._v("Requests")]),e._v(" "),s("p",[s("img",{attrs:{src:o(370),alt:"Requests"}})]),e._v(" "),s("h3",{attrs:{id:"limits-2"}},[e._v("Limits")]),e._v(" "),s("p",[s("img",{attrs:{src:o(371),alt:"Limits"}})]),e._v(" "),s("h2",{attrs:{id:"determining-the-right-values"}},[e._v("Determining the right values")]),e._v(" "),s("p",[e._v("There is no single right or wrong value, you need to calculate this based on your team's or microservice's usage and need. And this should be monitored explicitly for a period of time by the developers, to come to a final conclusion. For this, there are 2 options:")]),e._v(" "),s("ul",[s("li",[s("p",[e._v("Either you can do this manually through Grafana & Prometheus and look for container cpu & memory usage and then update Request & Limit manually.")])]),e._v(" "),s("li",[s("p",[e._v("Use Vertical Pod Autoscaler(Still in beta as of 17-04-2020)")])])]),e._v(" "),s("h2",{attrs:{id:"solution"}},[e._v("Solution")]),e._v(" "),s("h3",{attrs:{id:"don-t-overcommit-memory"}},[e._v("Don't overcommit memory")]),e._v(" "),s("ul",[s("li",[e._v("Implement mutating admission webhook")]),e._v(" "),s("li",[e._v("Set Requests = Limits")])]),e._v(" "),s("h3",{attrs:{id:"default-namespace-configuration"}},[e._v("Default Namespace Configuration")]),e._v(" "),s("p",[e._v("At the Namespace level, you can set up "),s("strong",[e._v("ResourceQuotas")]),e._v(" and "),s("strong",[e._v("LimitRanges")]),e._v(". ResourceQuotas are maxed limits for all pods in a namespace while LimitRanges are for individual pods in that resources. If a pod does not explicitly define a specific resource, they will inherit the assigned values. For example, If no default value is assigned, it will inherit the default limits.")]),e._v(" "),s("h3",{attrs:{id:"development-tier"}},[e._v("Development Tier")]),e._v(" "),s("h3",{attrs:{id:"production-tier"}},[e._v("Production Tier")]),e._v(" "),s("h2",{attrs:{id:"references"}},[e._v("References")]),e._v(" "),s("ul",[s("li",[e._v("https://sysdig.com/blog/kubernetes-limits-requests/")])]),e._v(" "),s("p",[e._v("CPU limit directly affects startup times.")]),e._v(" "),s("ul",[s("li",[e._v("https://medium.com/better-programming/the-kubernetes-quality-of-service-conundrum-eebbbb5f89cf")]),e._v(" "),s("li",[e._v("http://blog.kubecost.com/blog/requests-and-limits/")]),e._v(" "),s("li",[e._v("https://gravitational.com/blog/kubernetes-resource-planning/")])])])}),[],!1,null,null,null);t.default=i.exports}}]);